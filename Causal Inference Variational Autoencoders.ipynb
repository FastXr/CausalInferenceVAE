{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Causal Inference Variational Autoencoders.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMoz0Muqi+9rtFsxQ6uXlOe"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVq5HcoHIc8z","executionInfo":{"status":"ok","timestamp":1612700814819,"user_tz":-180,"elapsed":12806,"user":{"displayName":"Mehmet Enis İşgören","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gicb8XUxxdFYXBa2-Mw_1zF-3j_bpcmjDXC2IcmCw=s64","userId":"13379989964808381599"}},"outputId":"1909f752-99ba-49c5-f40e-88b6241dcd56"},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9QGPq68Ic_X","executionInfo":{"status":"ok","timestamp":1612700818933,"user_tz":-180,"elapsed":16832,"user":{"displayName":"Mehmet Enis İşgören","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gicb8XUxxdFYXBa2-Mw_1zF-3j_bpcmjDXC2IcmCw=s64","userId":"13379989964808381599"}},"outputId":"0a9ac890-2ab4-4162-da6d-fa47ed0fa542"},"source":["import torch\n","import torch as T\n","import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","RANDOM_SEED = 42  # Answer to the Ultimate Question of Life, the Universe, and Everything\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f6e9fd5bc00>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"UBkxp7ASIdB1"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mStKwbXaIdEG"},"source":["class Net(torch.nn.Module):\n","    def __init__(self, n_feature, n_hidden, n_output):\n","        super(Net, self).__init__()\n","        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n","        # Consider adding another layer\n","        self.hidden_2 = torch.nn.Linear(n_hidden, n_hidden)\n","        self.predict = torch.nn.Linear(n_hidden, n_output)\n","\n","    def forward(self, x):\n","        x = F.relu(self.hidden(x))\n","        x = F.relu(self.hidden_2(x))\n","        x = self.predict(x)\n","        return x\n","\n","\n","def train_network(train_x, train_y, net):\n","    net = net.train()  # set training mode\n","    loss_func = T.nn.MSELoss()  # mean squared error\n","    # loss = torch.sqrt(criterion(x, y))\n","\n","    optimizer = T.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","    X = train_x\n","    Y = train_y\n","    for b in range(10):\n","        optimizer.zero_grad()\n","        oupt = net(X.float())\n","        loss_obj = torch.sqrt(loss_func(oupt, Y.float()))  # Root mean squared error\n","        loss_obj.backward()\n","        optimizer.step()\n","    return net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVOyvV1SIdGf","executionInfo":{"status":"ok","timestamp":1612700818937,"user_tz":-180,"elapsed":16826,"user":{"displayName":"Mehmet Enis İşgören","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gicb8XUxxdFYXBa2-Mw_1zF-3j_bpcmjDXC2IcmCw=s64","userId":"13379989964808381599"}},"outputId":"2b519470-f99f-49a8-afb4-462044edabee"},"source":["%cd /content/drive/MyDrive/Colab\\ Notebooks/Causal_Inference_Final"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Causal_Inference_Final\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6Bc9Fv-IdIw","executionInfo":{"status":"ok","timestamp":1612700819182,"user_tz":-180,"elapsed":17064,"user":{"displayName":"Mehmet Enis İşgören","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gicb8XUxxdFYXBa2-Mw_1zF-3j_bpcmjDXC2IcmCw=s64","userId":"13379989964808381599"}},"outputId":"4df1a17a-d296-48f9-b35b-6fc4097c6300"},"source":["!ls ./data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["boston.csv  concrete.xls  energy.xlsx\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4dcI1X6xIoCh"},"source":[" def dataset_prep(df):\n","    label_column = df.columns[-1]\n","    train, test = train_test_split(df, test_size=0.1)\n","\n","    x_train = train.loc[:, train.columns != label_column]\n","    y_train = train.loc[:, train.columns == label_column]\n","\n","    x_test = test.loc[:, test.columns != label_column]\n","    y_test = test.loc[:, test.columns == label_column]\n","    return torch.from_numpy(x_train.values).float(), torch.from_numpy(y_train.values).float(), torch.from_numpy(x_test.values).float(), torch.from_numpy(y_test.values).float()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gusovcTYrySA"},"source":["class RegressionDataset(Dataset):\n","    def __init__(self, x_df, y_df):\n","        # for batch, (x, y) in enumerate(dataloader):\n","        self.x = x_df\n","        self.y = y_df \n","        \n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, idx): \n","        return self.x[idx], self.y[idx]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88QbpOMUt6bz"},"source":["def data_loader_pref(df):\n","    x_train, y_train, x_test, y_test = dataset_prep(df)\n","    train_dataset = RegressionDataset(x_train, y_train)\n","    trainloader = DataLoader(train_dataset, batch_size=16)\n","    test_dataset = RegressionDataset(x_test, y_test)\n","    testloader = DataLoader(test_dataset, batch_size=16)\n","    return trainloader, testloader, len(train_dataset[0][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHqjfdD-KT06"},"source":["def train_linear_regressor(train_loader, test_loader, n_dimensions):\n","    net = Net(n_feature=n_dimensions, n_hidden=64, n_output=1)\n","    net = net.train()  # set training mode\n","    loss_func = T.nn.MSELoss()  # mean squared error\n","    # loss = torch.sqrt(criterion(x, y))\n","\n","    optimizer = T.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","  \n","    for epoch in range(10):\n","        for batch, (x, y) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            oupt = net(x)\n","            loss_obj = torch.sqrt(loss_func(oupt, y))  # Root mean squared error\n","            loss_obj.backward()\n","            optimizer.step()\n","    total_loss = 0\n","    for batch, (x, y) in enumerate(test_loader):\n","        out = net(x)\n","        loss_obj = torch.sqrt(loss_func(out, y))\n","        total_loss += loss_obj.item()\n","\n","    return total_loss/len(test_loader)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbAmnvLDS0eY"},"source":["boston_df = pd.read_csv(\"./data/boston.csv\",header=None, delim_whitespace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrGF4I1zS9YL"},"source":["concrete_ = pd.read_excel(\"./data/concrete.xls\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6d0cnHnTbwN"},"source":["energy = pd.read_excel(\"./data/energy.xlsx\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPWIIV2oTyuf"},"source":["def linear_regressor_wrapper(df):\n","    train_loader, test_loader, n_dimensions = data_loader_pref(df)\n","    eval_loss = train_linear_regressor(train_loader, test_loader, n_dimensions)\n","    return eval_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3ssj3IMUO-c","executionInfo":{"status":"ok","timestamp":1612700942559,"user_tz":-180,"elapsed":1421,"user":{"displayName":"Mehmet Enis İşgören","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gicb8XUxxdFYXBa2-Mw_1zF-3j_bpcmjDXC2IcmCw=s64","userId":"13379989964808381599"}},"outputId":"32e5255a-f517-48da-ba97-6dd8611c8bf1"},"source":["boston_loss = linear_regressor_wrapper(boston_df)\n","print('Boston RMSE Loss: {}'.format(boston_loss))\n","energy_loss = linear_regressor_wrapper(energy)\n","print('Energy RMSE Loss: {}'.format(energy_loss))\n","concrete_loss = linear_regressor_wrapper(concrete_)\n","print('Concrete RMSE Loss: {}'.format(concrete_loss))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Boston RMSE Loss: 19.565793991088867\n","Energy RMSE Loss: 22.410636520385744\n","Concrete RMSE Loss: 37.00589043753488\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3oZm0n_DIoKP"},"source":["class Latent_Confounder_Encoder(nn.Module):\n","    def __init__(self, n_dimensions, output_dimension, hidden_size=64):\n","        super().__init__()\n","        self.first_net = Net(n_dimensions, hidden_size, output_dimension)\n","        self.second_net =Net(n_dimensions, hidden_size, output_dimension)\n","\n","        self.normal = torch.distributions.Normal\n","    def forward(self, z):\n","        first_out = self.first_net(z)\n","        second_out = self.second_net(z)\n","\n","        return self.normal(first_out, second_out)\n","\n","\n","class Encoder_Decoder_Model(nn.Module):\n","    def __init__(self, n_dimensions, output_dimension, hidden_size=64):\n","        super().__init__()\n","        self.first_net = Net(n_dimensions, hidden_size, n_dimensions)\n","        self.second_net = Net(n_dimensions, hidden_size, output_dimension)\n","        self.third_net = Net(n_dimensions, hidden_size, output_dimension)\n","\n","        self.normal = torch.distributions.Normal\n","\n","    def forward(self,x, y=None):\n","        if y != None:\n","            x = torch.cat((x, y), dim=1)\n","        # N(z| g 6 ∘ g 5 ([x, t]), g 7 ∘ g 5 ([x, t]) )\n","        # N(y| g 2 ∘ g 1 ([z, t]), g 3 ∘ g 1 ([z, t]) )\n","        common_out = self.first_net(x)\n","        return self.normal(self.second_net(common_out), self.third_net(common_out))\n","\n","        #      I guess this will be Z \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifznFCyxIoMe"},"source":["class GenerativeModel(nn.Module):\n","    def __init__(self,n_dimensions, hidden_size=64):\n","        super().__init__()\n","\n","        self.decoder_y = Encoder_Decoder_Model(n_dimensions=n_dimensions+1,\n","                                               output_dimension=1,\n","                                               hidden_size=hidden_size)\n","        \n","        self.latent_x = Latent_Confounder_Encoder(n_dimensions=n_dimensions,\n","                                                  output_dimension=n_dimensions)\n","        \n","        self.latent_t = Latent_Confounder_Encoder(n_dimensions=n_dimensions, \n","                                                  output_dimension=1)\n","\n","\n","    def forward(self, z, t):\n","        x_encoded = self.latent_x(z) # Why are this necessary ?\n","\n","        t_encoded = self.latent_t(z) # Why are this necessary ?\n","\n","        y_decoded = self.decoder_y(z, t)\n","\n","        return y_decoded, t_encoded, x_encoded\n","\n","class InferenceModel(nn.Module):\n","    def __init__(self,n_dimensions, hidden_size=64):\n","        super().__init__()\n","        self.encoder_z = Encoder_Decoder_Model(n_dimensions=n_dimensions,\n","                                               output_dimension=n_dimensions-1, \n","                                               hidden_size=hidden_size)\n","        # q(z|x, t) = N(z| g 6 ∘ g 5 ([x, t]), g 7 ∘ g 5 ([x, t]) )\n","    \n","\n","    def forward(self, x, t):\n","        z_encoded = self.encoder_z(x, t)\n","\n","        return z_encoded\n","        \n","\n","class VAE(nn.Module):\n","    def __init__(self,n_dimensions, hidden_size=64):\n","        super().__init__()\n","\n","\n","        self.generative = GenerativeModel(n_dimensions=n_dimensions, \n","                                        hidden_size=hidden_size)\n","        self.inference = InferenceModel(n_dimensions=n_dimensions+1,\n","                                    hidden_size=hidden_size)\n","    def forward(self,x, t):\n","        z_decoded = self.inference(x, t)\n","\n","        y_decoded, t_encoded, x_encoded = self.generative(z_decoded.sample(), t)\n","\n","        return z_decoded, y_decoded, t_encoded, x_encoded\n","\n","        \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dfud-GDN8KCb"},"source":["def log_norm(x, mu, std):\n","    \"\"\"Compute the log pdf of x,\n","    under a normal distribution with mean mu and standard deviation std.\"\"\"\n","    \n","    return torch.mean(-0.5 * torch.log(2*np.pi*std**2) -(0.5 * (1/(std**2))* (x-mu)**2))\n","\n","def elbo_loss(y_decoded, y, z_decoded, prior_z):\n","    # Fix this loss  +(torch.square(y_decoded.variance) / torch.square(y_decoded.mean))\n","   \n","    return torch.sum(torch.distributions.kl.kl_divergence(z_decoded, prior_z) - log_norm(y_decoded.sample(), y_decoded.mean, torch.sqrt(y_decoded.variance)) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dyb7RDQWbqbb"},"source":["class VAEDataset(Dataset):\n","    def __init__(self, x_df,t_df, y_df):\n","        # for batch, (x, y) in enumerate(dataloader):\n","        self.x = x_df\n","        self.t = t_df\n","        self.y = y_df \n","        \n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, idx): \n","        return self.x[idx], self.t[idx], self.y[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zd7qWCV5h0up"},"source":[" def vae_dataset_prep(df, t_index):\n","    label_column = df.columns[-1]\n","    treatment_column = df.columns[t_index]\n","    train, test = train_test_split(df, test_size=0.1)\n","\n","    x_train = train.loc[:, ~train.columns.isin([treatment_column, label_column])]\n","    t_train = train.loc[:, train.columns == treatment_column]\n","    y_train = train.loc[:, train.columns == label_column]\n","\n","    # x_test = test.loc[:, test.columns != label_column]\n","    # y_test = test.loc[:, test.columns == label_column]\n","    return torch.from_numpy(x_train.values).float(), torch.from_numpy(t_train.values).float(), torch.from_numpy(y_train.values).float()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VG-4KMjKasSh"},"source":["def vae_data_loader_pref(df, t_index):\n","    x_train, t_train, y_train = vae_dataset_prep(df, t_index)\n","\n","    train_dataset = VAEDataset(x_train,t_train, y_train)\n","    trainloader = DataLoader(train_dataset, batch_size=16)\n","\n","    return trainloader, len(train_dataset[0][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zf5xPX8Nbl7P"},"source":["def train_vae(train_loader, n_dimensions):\n","    vae = VAE(n_dimensions=n_dimensions)\n","    vae = vae.train()  # set training mode\n","    loss_func = T.nn.MSELoss()  # mean squared error\n","    # loss = torch.sqrt(criterion(x, y))\n","    prior_z = torch.distributions.Normal(torch.zeros(n_dimensions), torch.eye(1))\n","\n","    optimizer = T.optim.Adam(vae.parameters(), lr=0.0001)\n","  \n","    for batch, (x, t, y) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        z_decoded, y_decoded, t_encoded, x_encoded = vae(x, t)\n","        loss_obj = elbo_loss(y_decoded, y, z_decoded, prior_z)  # Root mean squared error\n","        loss_obj.backward()\n","        optimizer.step()\n","\n","    maximum_value = train_loader.dataset.t.max()\n","    minimum_value = train_loader.dataset.t.min()\n","    treatment_range = maximum_value - minimum_value\n","    # Implement a function here to determine how to intervene to T\n","    # trainloader.dataset.t Test this\n","    total_effect = 0\n","    for batch, (x, t, y) in enumerate(train_loader):\n","        batch_size_ = x.shape[0]\n","        _, y_decoded_max, _, _ = vae(x, torch.stack([maximum_value] * batch_size_).reshape(batch_size_,1))\n","        _, y_decoded_min, _, _ = vae(x, torch.stack([minimum_value] * batch_size_).reshape(batch_size_,1))\n","        \n","        effect = torch.sum(torch.abs((y_decoded_max.mean - y_decoded_min.mean)))/ batch_size_\n","        total_effect += effect.item()\n","\n","    return total_effect / len(train_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"krJ4n-hbhV0K"},"source":["# train_loader, n_dimensions = vae_data_loader_pref(boston_df, t_index=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cyy1RNiXxEOy"},"source":["def calculate_treatment_effect(df):\n","    treatment_effect_dict = {}\n","    n_intervensions = len(df.columns)-1\n","    for intervention_index in range(n_intervensions):\n","        train_loader, n_dimensions = vae_data_loader_pref(df, t_index=intervention_index)\n","        ate_ = train_vae(train_loader, n_dimensions)\n","        treatment_effect_dict[intervention_index] = ate_\n","    return treatment_effect_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNFkWFp9omky"},"source":["boston_effect_dict = calculate_treatment_effect(boston_df)\n","energy_effect_dict = calculate_treatment_effect(energy)\n","concrete_effect_dict = calculate_treatment_effect(concrete_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"txlZB00XyM4C"},"source":[" def causal_dataset_prep(df, selected_indexes):\n","    label_column = df.columns[-1]\n","    selected_column = []\n","    for c_index in selected_indexes:\n","        selected_column.append(df.columns[c_index])\n","    train, test = train_test_split(df, test_size=0.1)\n","\n","    x_train = train.loc[:, train.columns.isin(selected_column)]\n","    y_train = train.loc[:, train.columns == label_column]\n","\n","    x_test = test.loc[:, test.columns.isin(selected_column)]\n","    y_test = test.loc[:, test.columns == label_column]\n","    return torch.from_numpy(x_train.values).float(), torch.from_numpy(y_train.values).float(), torch.from_numpy(x_test.values).float(), torch.from_numpy(y_test.values).float()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYoqruZq3pMN"},"source":["def index_selector(score_dict):\n","    sorted_scores = sorted(score_dict.values(), reverse=True)\n","    best_indexes =[]\n","    for i in range(3):\n","        for column_index, score in score_dict.items():\n","            if score == sorted_scores[i]:\n","                best_indexes.append(column_index)\n","    return best_indexes\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CD6V4zFearIq"},"source":["def causal_data_loader_pref(df, score_dict):\n","    selected_indexes = index_selector(score_dict)\n","    x_train, y_train, x_test, y_test = causal_dataset_prep(df, selected_indexes)\n","\n","    train_dataset = RegressionDataset(x_train, y_train)\n","    trainloader = DataLoader(train_dataset, batch_size=16)\n","    test_dataset = RegressionDataset(x_test, y_test)\n","    testloader = DataLoader(test_dataset, batch_size=16)\n","    return trainloader, testloader, len(train_dataset[0][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZCeLAKmarLE"},"source":["def causal_linear_regressor_wrapper(df, score_dict):\n","    train_loader, test_loader, n_dimensions = causal_data_loader_pref(df, score_dict)\n","    eval_loss = train_linear_regressor(train_loader, test_loader, n_dimensions)\n","    return eval_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-iV6N6rarNc","executionInfo":{"status":"ok","timestamp":1612701036445,"user_tz":-180,"elapsed":1434,"user":{"displayName":"Mehmet Enis İşgören","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gicb8XUxxdFYXBa2-Mw_1zF-3j_bpcmjDXC2IcmCw=s64","userId":"13379989964808381599"}},"outputId":"8e87d586-2a44-41ba-ddba-a42296ce28bd"},"source":["boston_loss = causal_linear_regressor_wrapper(boston_df, boston_effect_dict)\n","print('Boston Causal RMSE Loss: {}'.format(boston_loss))\n","energy_loss = causal_linear_regressor_wrapper(energy, energy_effect_dict)\n","print('Energy Causal RMSE Loss: {}'.format(energy_loss))\n","concrete_loss = causal_linear_regressor_wrapper(concrete_, concrete_effect_dict)\n","print('Concrete Causal RMSE Loss: {}'.format(concrete_loss))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Boston Causal RMSE Loss: 13.546119928359985\n","Energy Causal RMSE Loss: 23.12357940673828\n","Concrete Causal RMSE Loss: 16.555611065455846\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xexxhsRoKvoD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTbRPdG2L63v"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLxJy8bxL66D"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OxulSHhL7CD"},"source":[""],"execution_count":null,"outputs":[]}]}